import os
import dask.dataframe as dd
import pandas as pd
from glob import glob
from sklearn.preprocessing import LabelEncoder
import numpy as np
idx_datatypes = {
    'Dst Port': np.int64,
    'Protocol': np.int8,
    'Timestamp': object,
    'Flow Duration': np.int64,
    'Tot Fwd Pkts': np.int16,
    'Tot Bwd Pkts': np.int16,
    'TotLen Fwd Pkts': np.int32,
    'TotLen Bwd Pkts': np.int32,
    'Fwd Pkt Len Max': np.int32,
    'Fwd Pkt Len Min': np.int32,
    'Fwd Pkt Len Mean': np.float64,
    'Fwd Pkt Len Std': np.float64,
    'Bwd Pkt Len Max': np.float64,
    'Bwd Pkt Len Min': np.int16,
    'Bwd Pkt Len Mean': np.float64,
    'Bwd Pkt Len Std': np.float64,
    'Flow Byts/s': np.float64,
    'Flow Pkts/s': np.float64,
    'Flow IAT Mean': np.float64,
    'Flow IAT Std': np.float64,
    'Flow IAT Max': np.int64,
    'Flow IAT Min': np.int32,
    'Fwd IAT Tot': np.int32,
    'Fwd IAT Mean': np.float32,
    'Fwd IAT Std': np.float64,
    'Fwd IAT Max': np.int32,
    'Fwd IAT Min': np.int32,
    'Bwd IAT Tot': np.int32,
    'Bwd IAT Mean': np.float64,
    'Bwd IAT Std': np.float64,
    'Bwd IAT Max': np.int64,
    'Bwd IAT Min': np.int64,
    'Fwd PSH Flags': np.int8,
    'Bwd PSH Flags': np.int8,
    'Fwd URG Flags': np.int8,
    'Bwd URG Flags': np.int8,
    'Fwd Header Len': np.int32,
    'Bwd Header Len': np.int32,
    'Fwd Pkts/s': np.float64,
    'Bwd Pkts/s': np.float64,
    'Pkt Len Min': np.int16,
    'Pkt Len Max': np.int32,
    'Pkt Len Mean': np.float64,
    'Pkt Len Std': np.float64,
    'Pkt Len Var': np.float64,
    'FIN Flag Cnt': np.int8,
    'SYN Flag Cnt': np.int8,
    'RST Flag Cnt': np.int8,
    'PSH Flag Cnt': np.int8,
    'ACK Flag Cnt': np.int8,
    'URG Flag Cnt': np.int8,
    'CWE Flag Count': np.int8,
    'ECE Flag Cnt': np.int8,
    'Down/Up Ratio': np.int16,
    'Pkt Size Avg': np.float32,
    'Fwd Seg Size Avg': np.float32,
    'Bwd Seg Size Avg': np.float32,
    'Fwd Byts/b Avg': np.int8,
    'Fwd Pkts/b Avg': np.int8,
    'Fwd Blk Rate Avg': np.int8,
    'Bwd Byts/b Avg': np.int8,
    'Bwd Pkts/b Avg': np.int8,
    'Bwd Blk Rate Avg': np.int8,
    'Subflow Fwd Pkts': np.int16,
    'Subflow Fwd Byts': np.int32,
    'Subflow Bwd Pkts': np.int16,
    'Subflow Bwd Byts': np.int32,
    'Init Fwd Win Byts': np.int32,
    'Init Bwd Win Byts': np.int32,
    'Fwd Act Data Pkts': np.int16,
    'Fwd Seg Size Min': np.int8,
    'Active Mean': np.float64,
    'Active Std': np.float64,
    'Active Max': np.int32,
    'Active Min': np.int32,
    'Idle Mean': np.float64,
    'Idle Std': np.float64,
    'Idle Max': np.int64,
    'Idle Min': np.int64,
    'Label': object
}


def merge_data_by_labels():
    # === è¨­å®šåƒæ•¸ ===
    input_folders = ["CICIDS2017_improved", "CSECICIDS2018_improved"]
    output_folder = "labeled_data"
    time_column = "Timestamp"
    time_format = "%Y-%m-%d %H:%M:%S.%f"
    label_column = "Label"
    drop_columns = [
        'id', 'Flow ID', 'Attempted Category',
        'Src IP', 'Dst IP', 'Src Port'  # é¿å… overfitting
    ]

    # è‡ªå‹•å»ºç«‹åˆ†é¡è³‡æ–™è³‡æ–™å¤¾
    os.makedirs(output_folder, exist_ok=True)

    # === ç¬¬ä¸€æ­¥ï¼šä¾é¡åˆ¥åˆ†é¡å„²å­˜ ===
    for folder in input_folders:
        for file in os.listdir(folder):
            if file.endswith(".csv"):
                print(f"è™•ç†æª”æ¡ˆï¼š{file}")
                path = os.path.join(folder, file)

                # è®€å–è³‡æ–™
                df = dd.read_csv(path, assume_missing=True)
                df = df[[col for col in df.columns if col not in drop_columns]]

                # è½‰æ› Timestampï¼Œè‡ªå‹•åˆ¤æ–·æ ¼å¼
                df[time_column] = dd.to_datetime(
                    df[time_column], format=time_format, errors="coerce")
                df = df.dropna(subset=[time_column, label_column])

                # å–å¾—æ‰€æœ‰ label é¡åˆ¥
                labels = df[label_column].dropna().unique().compute().tolist()

                # ä¾é¡åˆ¥åˆ†é¡å„²å­˜
                for label in labels:
                    print(label)
                    sub_df = df[df[label_column] == label]
                    label_filename = os.path.join(
                        output_folder, f"{label}.csv"
                    )

                    # å¯«å…¥æˆ–é™„åŠ 
                    if os.path.exists(label_filename):
                        sub_df.to_csv(label_filename, index=False,
                                      single_file=True, mode='a', header=False)
                    else:
                        sub_df.to_csv(label_filename, index=False,
                                      single_file=True)


def sample_benign():
    # è®€å–åŸå§‹ BENIGN è³‡æ–™ï¼ˆå®‰å…¨è¨˜æ†¶é«”æ–¹å¼ï¼‰
    df = dd.read_csv("mess_data/BENIGN_30percent.csv")

    # éš¨æ©ŸæŠ½æ¨£ä¿ç•™ 15% è³‡æ–™
    df_sampled = df.sample(frac=0.5, random_state=520)

    # è¼¸å‡ºç‚ºæ–°çš„ CSV
    df_sampled.to_csv("mess_data/BENIGN_15percent.csv",
                      index=False, single_file=True)
    print("âœ… å·²æˆåŠŸä¿ç•™ 15% BENIGN")


def merge_attempted():

    # === è·¯å¾‘è¨­å®š ===
    benign_sampled_path = "mess_data/BENIGN_15percent.csv"
    attempted_pattern = "mess_data/*Attempted*.csv"
    output_path = "labeled_data/BENIGN_15_final.csv"

    # === è®€å–æ¬ æ¡æ¨£å¾Œçš„ BENIGN
    df_benign = dd.read_csv(benign_sampled_path)

    # === è®€å–æ‰€æœ‰ Attempted æª”æ¡ˆä¸¦æ”¹ Label
    attempted_list = []
    for path in glob(attempted_pattern):
        print(f"ğŸ“¥ è®€å– Attemptedï¼š{path}")
        df_attempted = dd.read_csv(path)
        df_attempted["Label"] = "BENIGN"
        attempted_list.append(df_attempted)

    # === åˆä½µ BENIGN + æ‰€æœ‰ Attempted
    df_all_benign = dd.concat([df_benign] + attempted_list)

    # === è¼¸å‡ºç‚ºå–®ä¸€ Parquet æª”æ¡ˆï¼ˆæ›´å¿«æ›´çœè¨˜æ†¶é«”ï¼‰
    df_all_benign.to_csv(output_path, index=False, single_file=True)
    print("âœ… å·²æˆåŠŸç”¢å‡ºæ•´åˆå¾Œçš„ BENIGN + Attempted è³‡æ–™ï¼")


def sort_split_data():

    # === åƒæ•¸è¨­å®š ===
    input_folder = "labeled_data"
    time_column = "Timestamp"
    label_column = "Label"
    time_format = "%Y-%m-%d %H:%M:%S.%f"
    train_ratio = 0.8

    # çµæœæš«å­˜å€
    train_list, val_list, label_stats = [], [], []

    # === è™•ç†æ¯å€‹ CSV æª”æ¡ˆ ===
    for path in glob(f"{input_folder}/*.csv"):
        filename = os.path.basename(path)

        print(f"ğŸ“‚ è™•ç†ï¼š{filename}")
        label = filename.replace(".csv", "")

        if label == 'BENIGN_15_final':
            label = 'BENIGN'
            print(label)

        # è®€å…¥è³‡æ–™ï¼ˆè¨˜æ†¶é«”å®‰å…¨ï¼‰
        ddf = dd.read_csv(path)

        # æ™‚é–“è½‰æ›èˆ‡æ¸…æ´—
        ddf[time_column] = dd.to_datetime(
            ddf[time_column], format=time_format, errors="coerce")
        ddf = ddf.dropna(subset=[time_column])
        ddf = ddf.sort_values(by=time_column)

        total_rows = len(ddf)
        print(total_rows)
        if total_rows < 2:
            print(f"âš ï¸ è·³é {label}ï¼Œè³‡æ–™å¤ªå°‘")
            continue

        # åˆ‡åˆ† 80 / 20
        split_idx = int(total_rows * train_ratio)
        train_df = ddf.head(split_idx)
        val_df = ddf.tail(total_rows - split_idx)

        # å„²å­˜
        train_list.append(train_df)
        val_list.append(val_df)

        # çµ±è¨ˆåˆ†å¸ƒ
        label_stats.append({
            "Label": label,
            "Train Count": len(train_df),
            "Val Count": len(val_df),
            "Total": total_rows
        })

    # === åˆä½µè¨“ç·´é›†èˆ‡é©—è­‰é›† ===
    train_all = pd.concat(train_list).reset_index(drop=True)
    val_all = pd.concat(val_list).reset_index(drop=True)

    # å»é™¤ Timestamp æ¬„ä½
    if time_column in train_all.columns:
        train_all = train_all.drop(columns=[time_column])
    if time_column in val_all.columns:
        val_all = val_all.drop(columns=[time_column])

    # === Label ç·¨ç¢¼ï¼ˆå°‡æ–‡å­—è½‰ç‚ºæ•´æ•¸ï¼‰
    encoder = LabelEncoder()
    train_all[label_column] = encoder.fit_transform(train_all[label_column])
    val_all[label_column] = encoder.transform(val_all[label_column])

    # å„²å­˜ label å°ç…§è¡¨ï¼ˆè½‰æ›è¡¨ï¼‰
    label_map_df = pd.DataFrame({
        "Label": encoder.classes_,
        "Encoded": range(len(encoder.classes_))
    })
    label_map_df.to_csv("label_encoding_map.csv", index=False)

    # === è¼¸å‡ºçµæœ ===
    train_all.to_csv("final_train.csv", index=False)
    val_all.to_csv("final_val.csv", index=False)
    pd.DataFrame(label_stats).to_csv("label_distribution.csv", index=False)

    print("âœ… å®Œæˆï¼å·²ç”¢å‡ºï¼š")
    print("  final_train.csvã€final_val.csvï¼ˆå«ç·¨ç¢¼ Labelï¼‰")
    print("  label_distribution.csvï¼ˆæ¯é¡ç­†æ•¸çµ±è¨ˆï¼‰")
    print("  label_encoding_map.csvï¼ˆLabel ç·¨ç¢¼å°ç…§è¡¨ï¼‰")


def get_csv_size():
    folder = "labeled_data"
    csv_files = [f for f in os.listdir(folder) if f.endswith(".csv")]

    total_size = 0
    file_sizes = []

    for f in csv_files:
        path = os.path.join(folder, f)
        size_mb = os.path.getsize(path) / (1024 * 1024)  # bytes â†’ MB
        total_size += size_mb
        file_sizes.append((f, round(size_mb, 2)))

    # è¼¸å‡ºæ¯å€‹æª”æ¡ˆå¤§å°
    print("æ¯å€‹ Label æª”æ¡ˆå¤§å°ï¼ˆå–®ä½ï¼šMBï¼‰")
    for fname, size in sorted(file_sizes, key=lambda x: -x[1]):
        print(f"{fname:40s}  {size:7.2f} MB")

    # è¼¸å‡ºç¸½å¤§å°
    print(f"ç¸½å¤§å°ï¼š{round(total_size, 2)} MB")


def get_demo(path):
    df = pd.read_csv(path, nrows=5)
    df.to_csv(f'{path}_demo.csv', index=False)
    print('demo saved')
    print(df)  # é è¨­å°å‰ 5 è¡Œ


def clean_inf(input_path, output_path=None):
    """
    æ¸…ç† infï¼Œä½†ä¿ç•™ NaN çµ¦ XGBoost è‡ªå‹•è™•ç†
    """
    print(f"ğŸ“‚ è¼‰å…¥æª”æ¡ˆï¼š{input_path}")
    df = pd.read_csv(input_path)

    original_rows = len(df)

    # å¼·åˆ¶è½‰æ•¸å€¼ï¼Œå°‡ "inf" å­—ä¸²ç­‰ç„¡æ•ˆå€¼è½‰æˆ NaN æˆ– inf
    df = df.apply(pd.to_numeric, errors='coerce')

    # åˆªé™¤å«æœ‰ inf çš„ rowï¼ˆä¿ç•™ NaNï¼‰
    mask_inf = np.isinf(df).any(axis=1)
    cleaned_df = df[~mask_inf].copy()

    print(f"åŸå§‹åˆ—æ•¸ï¼š{original_rows}")
    print(f"åˆªé™¤ inf åˆ—æ•¸ï¼š{mask_inf.sum()}")
    print(f"ä¿ç•™ NaNï¼Œæ¸…ç†å¾Œå‰©ä¸‹ï¼š{len(cleaned_df)} åˆ—")

    if output_path is None:
        base, ext = os.path.splitext(input_path)
        output_path = base + "_cleaned.csv"

    cleaned_df.to_csv(output_path, index=False)
    print(f"âœ… è¼¸å‡ºå®Œæˆï¼š{output_path}")


def check_data(df, name="Data"):
    nan_count = df.isna().sum().sum()
    inf_count = np.isinf(df.to_numpy()).sum()
    print(f"\n--- check {name} ---")
    print(f"numbers of NaN: {nan_count}")
    print(f"numbers of inf: {inf_count}")

def mutli2bin():
    path = 'final_data/final_train_cleaned.csv'
    df = pd.read_csv(path, low_memory=False)
    print('read')
    # å°‡ Label != 0 çš„å…¨è¨­ç‚º 1
    df["Label"] = df["Label"].apply(lambda x: 0 if x == 0 else 1)

    df.to_csv(f'{path}_binary.csv', index=False)
    df_demo = df.head()
    df_demo.to_csv(f'{path}_binary_demo.csv', index=False)


# get_csv_size()
# sample_benign()
# merge_attempted()
# sort_split_data()
# get_demo()
# clean_inf('final_train.csv', 'final_train_cleaned.csv')
# clean_inf('final_val.csv', 'final_val_cleaned.csv')
df = pd.read_csv("final_data/final_train_cleaned_binary.csv")
check_data(df)
df = pd.read_csv("final_data/final_val_cleaned_binary.csv")
check_data(df)
